# T5.1.0 11B model.
from __gin__ import dynamic_registration

import __main__ as train_script
import seqio
from t5x import adafactor
from t5x import models
from t5x.examples.t5 import network

from t5x import optimizers
from t5x import decoding
from t5.data import mixtures
from t5x import gin_utils
from t5x import models
from t5x import partitioning
from t5x import utils
from t5x import trainer
import optax

import tasks

MIXTURE_OR_TASK_NAME = %gin.REQUIRED
TASK_FEATURE_LENGTHS = %gin.REQUIRED
TRAIN_STEPS = %gin.REQUIRED
MODEL_DIR = %gin.REQUIRED
USE_CACHED_TASKS = True

# DEPRECATED: Import the this module in your gin file.
MIXTURE_OR_TASK_MODULE = None
SHUFFLE_TRAIN_EXAMPLES = True

# HW RNG is faster than SW, but has limited determinism. Most notably it is not deterministic across different submeshes.
USE_HARDWARE_RNG = False
RANDOM_SEED = None  # None always uses faster, hardware RNG

MIXTURE_OR_TASK_NAME = "pile"
TASK_FEATURE_LENGTHS = {"inputs": 512, "targets": 512}  # sequence length; shorter will be truncated
TRAIN_STEPS = 200000

train_script.train:
  # from pretrain.gin
  model = %MODEL  # imported from separate gin file
  model_dir = %MODEL_DIR
  train_dataset_cfg = @train/utils.DatasetConfig()
  train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()
  checkpoint_cfg = @utils.CheckpointConfig()
  partitioner = @partitioning.PjitPartitioner()
  trainer_cls = @trainer.Trainer
  total_steps = %TRAIN_STEPS
  summarize_config_fn = @gin_utils.summarize_gin_config

  # TODO HELEN TURN THIS BACK ON LATER; TURNED OFF JUST FOR ITERATION SPEED
  run_eval_before_training = False #True  # Assert that `loss_per_nonpadding_target_token` makes sense given the vocabulary size
  eval_period = 20
  eval_steps = 20
  random_seed = 0
  use_hardware_rng = True
  infer_eval_dataset_cfg = None # @infer_eval/utils.DatasetConfig()
  inference_evaluator_cls = None #@seqio.Evaluator

BATCH_SIZE = 18 #24 #48 #15  # global batch size

partitioning.PjitPartitioner:
  num_partitions = 8  # change to 1 for running on a single GPU
  # model_parallel_submesh = (1, 1, 1, 1)  # model_parallel_submesh and are exclusive of each other
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

# ------------------- Loss HParam ----------------------------------------------
Z_LOSS = 0.0001
LABEL_SMOOTHING = 0.0
LOSS_NORMALIZING_FACTOR = None

# Vocabulary (shared by encoder and decoder)
VOCABULARY = @seqio.SentencePieceVocabulary()
seqio.SentencePieceVocabulary.sentencepiece_model_file = "gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model"

# ------------------- Optimizer ------------------------------------------------
# Gin configuration makes it easy by simply importing any available optimizer in t5x/optimizers module.
# Note the optimizers in t5x/optimizers are wrapped version of optimizers implemented in optax.

# In this case, we choose to switch to the AdamW optimizer with gradient clip.
#  `learning_rate` is set by `Trainer.learning_rate_fn`.
OPTIMIZER = @optimizers.chain()

optimizers.chain:
  transformations = [@optax.clip(), @optax.adamw()]

optax.clip:
  max_delta = 1.0

optax.adamw:
  # Unlike Adafactor, most optimizers require to specify
  # `learning_rate`. `learning_rate` accepts a float number (e.g., 1e-4) or
  # a schedule function, which should take an argument `step` and output
  # a learning rate for that step.
  # As for choices of schedule functions, we can either use T5x
  # learning rate scheduler, i.e., utils.create_learning_rate_scheduler, or
  # optax's native schedule functions, e.g., warmup_cosine_decay_schedule.
  learning_rate = @optax.warmup_cosine_decay_schedule()

optax.warmup_cosine_decay_schedule:
  init_value = 0.0
  peak_value = 1e-4
  warmup_steps = 1000
  decay_steps = %TRAIN_STEPS
  end_value = 0.0

utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 10000  # 10k to keep consistent with T5/MTF defaults.

# ------------------- Model ----------------------------------------------------
MODEL = @models.EncoderDecoderModel()
models.EncoderDecoderModel:
  module = @network.Transformer()
  input_vocabulary = %VOCABULARY
  output_vocabulary = %VOCABULARY
  optimizer_def = %OPTIMIZER
  z_loss = %Z_LOSS
  label_smoothing = %LABEL_SMOOTHING
  loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR

# ------------------- Network specification ------------------------------------
network.Transformer.config = @network.T5Config()
network.T5Config:
  vocab_size = 32128  # vocab size rounded to a multiple of 128 for TPU efficiency
  dtype = 'bfloat16'
  emb_dim = 1024
  num_heads = 128
  num_encoder_layers = 24
  num_decoder_layers = 24
  head_dim = 128
  mlp_dim = 65536
  dropout_rate = 0.0
  logits_via_embedding = True

train/utils.DatasetConfig:

  # from pretrain.gin
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'train'
  batch_size = %BATCH_SIZE
  shuffle = %SHUFFLE_TRAIN_EXAMPLES
  seed = None  # use a new seed each run/restart
  use_cached = %USE_CACHED_TASKS
  pack = True
  module = %MIXTURE_OR_TASK_MODULE

  use_cached = False
  pack = True
  seed = 0

train_eval/utils.DatasetConfig:

  # from pretrain.gin
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'validation'
  batch_size = %BATCH_SIZE
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = True
  module = %MIXTURE_OR_TASK_MODULE

  use_cached = False
  pack = True
  seed = 0

# Comment this back in later
#infer_eval/utils.DatasetConfig:
#  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
#  task_feature_lengths = %TASK_FEATURE_LENGTHS  # set to None to compute max
#  split = "validation"
#  seed = 0
#  batch_size = %BATCH_SIZE
#  shuffle = False
#  use_cached = False
# """

utils.CheckpointConfig:
  restore = @utils.RestoreCheckpointConfig()
  save = @utils.SaveCheckpointConfig()
utils.RestoreCheckpointConfig:
  path = []  # initialize from scratch

utils.SaveCheckpointConfig:
  dtype = 'float32'
  keep = 1  # set to None to keep all checkpoints
  save_dataset = False  # don't checkpoint dataset state
  period = 1000  # checkpoint frequency

trainer.Trainer:
  num_microbatches = None
  learning_rate_fn = @utils.create_learning_rate_scheduler()

