# Decoder-only model (Base) with 109632768 parameters. Runs on 8 A100s (for bfloat16), designed to mirror training smallest GPT on a v3-8.
# Run command: python t5x/train.py --gin_file="t5x/examples/decoder_only/examples/tiny_gpu_8x.gin" --gin.MODEL_DIR=\"${MODEL_DIR}\" --tfds_data_dir=${TFDS_DATA_DIR}

from __gin__ import dynamic_registration

import __main__ as train_script
import seqio

from t5x import adafactor
from t5x import optimizers
from t5x import decoding
from t5.data import mixtures
from t5x import gin_utils
from t5x import models
from t5x.examples.decoder_only import network
from t5x import partitioning
from t5x import utils
from t5x import trainer
import optax

import tasks

from t5x.data import gpt2_encoder

MIXTURE_OR_TASK_NAME = %gin.REQUIRED
TASK_FEATURE_LENGTHS = %gin.REQUIRED
TRAIN_STEPS = %gin.REQUIRED
MODEL_DIR = %gin.REQUIRED
BATCH_SIZE = 72 #64  # global batch size
USE_CACHED_TASKS = True

# DEPRECATED: Import the this module in your gin file.
MIXTURE_OR_TASK_MODULE = None
SHUFFLE_TRAIN_EXAMPLES = True

# HW RNG is faster than SW, but has limited determinism. Most notably it is not deterministic across different submeshes.
USE_HARDWARE_RNG = False
RANDOM_SEED = None  # None always uses faster, hardware RNG

MIXTURE_OR_TASK_NAME = "wmt_t2t_ende_v003"
MIXTURE_OR_TASK_NAME = "pile"
TASK_FEATURE_LENGTHS = {"inputs": 1024, "targets": 1024}  # sequence length; shorter will be truncated
TRAIN_STEPS = 200000

train_script.train:
  # from pretrain.gin
  model = %MODEL  # imported from separate gin file
  model_dir = %MODEL_DIR
  train_dataset_cfg = @train/utils.DatasetConfig()
  train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()
  checkpoint_cfg = @utils.CheckpointConfig()
  partitioner = @partitioning.PjitPartitioner()
  trainer_cls = @trainer.Trainer
  total_steps = %TRAIN_STEPS
  summarize_config_fn = @gin_utils.summarize_gin_config

  run_eval_before_training = True  # Assert that `loss_per_nonpadding_target_token` makes sense given the vocabulary size
  eval_period = 5000
  eval_steps = 20
  random_seed = 0
  use_hardware_rng = True
  infer_eval_dataset_cfg = None # @infer_eval/utils.DatasetConfig()
  inference_evaluator_cls = None #@seqio.Evaluator

partitioning.PjitPartitioner:
  num_partitions = None #1 #2  # change to 1 for running on a single GPU
  model_parallel_submesh = (1, 1, 1, 1)  # model_parallel_submesh and are exclusive of each other
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

# ------------------- Loss HParam ----------------------------------------------
Z_LOSS = 0.0001
LABEL_SMOOTHING = 0.0
LOSS_NORMALIZING_FACTOR = None  # set to pretraining batch_size * target_token_length when fine-tuning the public T5 checkpoints (trained in T5 MeshTF)

# Vocabulary (shared by encoder and decoder)
VOCABULARY = @seqio.SentencePieceVocabulary()
seqio.SentencePieceVocabulary.sentencepiece_model_file = "gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model"

# Vocabulary, using GPT2 vocab
#VOCABULARY = @gpt2_encoder.GPT2Vocabulary()

# ------------------- Optimizer ------------------------------------------------
# Gin configuration makes it easy by simply importing any available optimizer in t5x/optimizers module.
# Note the optimizers in t5x/optimizers are wrapped version of optimizers implemented in optax.

# In this case, we choose to switch to the AdamW optimizer with gradient clip.
#  `learning_rate` is set by `Trainer.learning_rate_fn`.
OPTIMIZER = @optimizers.chain()

optimizers.chain:
  transformations = [@optax.clip(), @optax.adamw()]

optax.clip:
  max_delta = 1.0

optax.adamw:
  # Unlike Adafactor, most optimizers require to specify
  # `learning_rate`. `learning_rate` accepts a float number (e.g., 1e-4) or
  # a schedule function, which should take an argument `step` and output
  # a learning rate for that step.
  # As for choices of schedule functions, we can either use T5x
  # learning rate scheduler, i.e., utils.create_learning_rate_scheduler, or
  # optax's native schedule functions, e.g., warmup_cosine_decay_schedule.
  learning_rate = @optax.warmup_cosine_decay_schedule()

optax.warmup_cosine_decay_schedule:
  init_value = 0.0
  peak_value = 1e-4
  warmup_steps = 1000
  decay_steps = %TRAIN_STEPS
  end_value = 0.0

utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 10000  # 10k to keep consistent with T5/MTF defaults.

# ------------------- Model ----------------------------------------------------
MODEL = @models.DecoderOnlyModel()
models.DecoderOnlyModel:
  module = @network.DecoderWrapper()
  vocabulary = %VOCABULARY
  optimizer_def = %OPTIMIZER
  decode_fn = @decoding.temperature_sample
  z_loss = %Z_LOSS
  label_smoothing = %LABEL_SMOOTHING
  loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR

decoding.temperature_sample:
  temperature = 1.0
  topk = 40

# ------------------- Network specification ------------------------------------
network.DecoderWrapper.config = @network.TransformerConfig()
network.TransformerConfig:
  vocab_size = 32128  # vocab size rounded to a multiple of 128 for TPU efficiency
  dtype = 'bfloat16'
  emb_dim = 768
  num_heads = 12
  num_layers = 12
  head_dim = 64
  mlp_dim = 2048
  mlp_activations = ('gelu', 'linear')
  dropout_rate = 0.0
  logits_via_embedding = True

train/utils.DatasetConfig:

  # from pretrain.gin
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'train'
  batch_size = %BATCH_SIZE
  shuffle = %SHUFFLE_TRAIN_EXAMPLES
  seed = None  # use a new seed each run/restart
  use_cached = %USE_CACHED_TASKS
  pack = True
  module = %MIXTURE_OR_TASK_MODULE

  use_cached = False
  pack = True
  seed = 0

train_eval/utils.DatasetConfig:

  # from pretrain.gin
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'validation'
  batch_size = %BATCH_SIZE
  shuffle = False
  seed = 42
  use_cached = %USE_CACHED_TASKS
  pack = True
  module = %MIXTURE_OR_TASK_MODULE

  use_cached = False
  pack = True
  seed = 0

# Comment this back in later
#infer_eval/utils.DatasetConfig:
#  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
#  task_feature_lengths = %TASK_FEATURE_LENGTHS  # set to None to compute max
#  split = "validation"
#  seed = 0
#  batch_size = %BATCH_SIZE
#  shuffle = False
#  use_cached = False
# """

#seqio.Evaluator:
#  logger_cls = [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]
#  num_examples = 500  # Set to None to use all examples in the infer_eval dataset.
#  use_memory_cache = True

utils.CheckpointConfig:
  restore = @utils.RestoreCheckpointConfig()
  save = @utils.SaveCheckpointConfig()
utils.RestoreCheckpointConfig:
  path = []  # initialize from scratch

utils.SaveCheckpointConfig:
  dtype = 'float32'
  keep = 1  # set to None to keep all checkpoints
  save_dataset = False  # don't checkpoint dataset state
  period = 1000  # checkpoint frequency

trainer.Trainer:
  num_microbatches = None
  learning_rate_fn = @utils.create_learning_rate_scheduler()

# `num_decodes` is equivalent to a beam size in a beam search decoding.
models.DecoderOnlyModel.predict_batch_with_aux.num_decodes = 8
models.DecoderOnlyModel.inputs_bidirectional_attention = True
